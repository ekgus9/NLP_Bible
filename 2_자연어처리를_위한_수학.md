# 2 자연어 처리를 위한 수학

### 2.1 확률의 기초


확률 : 어떠한 사건이 발생할 수 있는 가능성을 수치로 나타낸 것

1. 확률 변수

표본 공간 sample space : 사건이 발생할 모든 경우 (동전 두 개를 던지는 사건에서 '앞앞, 뒤뒤, 앞뒤, 뒤앞'으로 4)



확률 변수 random variable : 어떠한 사건을 실수 표현으로 매칭시키는 일종의 함수 X (동전 두개를 던져 앞면이 나오는 횟수)



P(X=사건) = 확률



(확률 변수를 사용한 확률 함수를 모두 더하면 1)


2. 확률 변수와 확률 분포

이산 확률 변수 : 확률 변수 X가 취할 수 있는 값들이 이산적으로 셀 수 있는 경우



확률 분포 : 확률 변수가 특정한 값을 가질 확률을 나타내는 함수



이산 확률 분포 : 확률 변수가 이산 확률 변수 ex. 이항 분포, 다항 분포 등



확률 질량 함수 : f(x) = P(X=x)



-



연속 확률 변수 : 확률 변수 X가 취할 수 있는 값들이 어떤 범위로 주어지는 경우 ex. 사람의 키, 체중 등



연속 확률 분포, 확률 밀도 함수



3. 조건부 확률

: 어떤 사상 A가 일어났다고 가정한 상태에서 사상 B가 일어날 확률 P(B|A) = P(A∩B) / P(A)



만약 A와 B가 독립 P(B|A) = P(B)



베이즈 정리 : B1, ... , Bn의 사상들이 서로 겹치지 않게 분할 되어 있을때 어떤 사상 A



![image](https://user-images.githubusercontent.com/89879599/167562761-4592367f-58c5-4763-b491-07fe176253b7.png)



4. 기댓값과 분산

가중 평균 : 모든 면이 동일한 확률로 나오지 않는다.



기댓값 : 나오게 될 숫자에 대한 예상, 확률 변수의 평균 E(X)



분산 : 확률 분포에서 확률 변수들이 퍼져있는 정도



표준편차 : 분산의 제곱근



5. 이항분포, 다항분포, 정규분포


이항분포 : 확률이 p인 베르누이 시행을 n번 반복시행할 때 출현 횟수를 나타내는 확률변수 X의 분포



![image](https://user-images.githubusercontent.com/89879599/167568061-a80d60e2-5807-46b5-bb48-3f3006ea90cf.png)



다항분포 : 이항분포의 일반화



![image](https://user-images.githubusercontent.com/89879599/167568221-05ab3f8a-d29e-42b3-afb9-b7f9e143fb21.png)



정규분포 : 가우시안 분포



![image](https://user-images.githubusercontent.com/89879599/167568516-8f92a7a3-0eff-4cac-8eaf-85405c42df7e.png)



### 2.2 MLE와 MAP

1. Maximum Likelihood Estimation(MLE)

![image](https://user-images.githubusercontent.com/89879599/167570275-e9dcc2ec-81eb-42b8-bd19-30f933523105.png)



(P(D|쎄타) : 쎄타가 주어졌을 때 D가 나올 확률들의 분포)



쎄타 헷의 미분을 통해 최적의 값을 구할 수 있다.



![image](https://user-images.githubusercontent.com/89879599/167571960-4ca102de-5225-4c73-9ffe-61257c4e64e5.png)



(편의를 위해 로그 입힘 -> 로그 미분 / ln x 미분 -> 1/x)



2. Maximum a Posteriori Estimation(MAP) -> 데이터가 적다면 유리

몇 번 던져보지 않고 결과 예측은 weird



![image](https://user-images.githubusercontent.com/89879599/167572971-8195d1f8-d088-4279-be3f-987e3e67a80c.png)



D가 나왔을때 그것이 사실일 확률



쎄타의 분포를 우리가 계산 결과에 넣을 수 있다. : 사전 지식



![image](https://user-images.githubusercontent.com/89879599/167574306-b12b7e2d-78f7-4e74-8ba3-5ace4c9bd3c2.png)



3. 예시

MLE : 게임을 하는 사람들 중 그 통장잔고가 나올 확률과 게임을 하지 않는 사람들 중 그 통장잔고가 나올 확률을 비교하여 둘 중 더 높은 확률로 선택



MAP : 통장잔고가 주어졌을 때, 그것이 게임을 하는 사람의 것일 확률과 게임을 하지 않는 사람의 것일 확률을 비교하여 둘 중 더 높은 확률로 선택하는 것
(이 경우에는 게임을 하는 사람과 게임을 하지 않는 사람의 비율 결정에 반영된다.)
 
### 3. 정보 이론과 엔트로피

1. 정보량

가정 1. 중요성 : 어떤 사건이 발생할 가능성이 낮을수록 그 사건은 많은 정보를 지닌다.



![image](https://user-images.githubusercontent.com/89879599/167575492-82a6bb58-4094-4de9-9bb5-555671601abe.png)



가정 2. 가법성 : 독립적인 두 정보량은 더하고 뺄 수 있다.



![image](https://user-images.githubusercontent.com/89879599/167575788-79296be9-0307-4c6f-b2e5-4ec512b958d6.png)



=> ![image](https://user-images.githubusercontent.com/89879599/167575985-3a07d119-be71-43f5-9e79-5717202029a4.png)



![image](https://user-images.githubusercontent.com/89879599/167576918-e836354f-8d5f-4734-ab4f-cb7af1b076f1.png)



(밑이 2인 로그 : 결과에 영향을 미치지 않는 단조 증가 함수)



2. 엔트로피


: 확률 변수 X의 표본공간에서 나타나는 모든 사상들의 정보량의 평균적인 기댓값을 의미 (정보의 불확실성 평가 -> 높을 수록 불확실성 높다)



![image](https://user-images.githubusercontent.com/89879599/167577865-58a90360-c83a-4126-be34-6880adb40443.png)



조건부 엔트로피 : 신용등급이 3일때 대출이 가능한가



![image](https://user-images.githubusercontent.com/89879599/167578234-18f7fa33-17b3-4692-b715-0c2af6102bba.png)



결합 엔트로피 : 두개의 확률변수에 대한 엔트로피



![image](https://user-images.githubusercontent.com/89879599/167580088-4bccfea3-804a-44f5-904b-dd92c230fc54.png)



3. KL-divergence, Perplexit

KL-divergence : 두 분포가 서로 얼마나 일치할 수 있는지 측정 (낮을수록 일치)



![image](https://user-images.githubusercontent.com/89879599/167581392-3eaf35bb-5938-40b4-ad7a-ef0ef76b8da1.png)



Perplexity(PPL) : 두 개의 언어 모델이 있을 때 이를 평가하기 위한 지표 
(단어 수로 정규화 된 테스트 데이터에 대한 확률의 역수로 계산)



![image](https://user-images.githubusercontent.com/89879599/167581696-a392baba-f48e-4418-b33f-e9e59595d7a3.png)

